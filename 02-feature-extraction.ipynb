{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f54bd1d9-e33c-4684-942e-aab424336090",
   "metadata": {},
   "source": [
    "# Feature Extraction (part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc913e-fb76-4e01-a64f-88e2e721bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import vocalpy as voc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838461d0-daa6-489c-af5d-907e2d854788",
   "metadata": {},
   "source": [
    "## Acoustic features\n",
    "\n",
    "In their simplest form, these area scalar value we compute *per frame* (time bin) of a spectrogram. For example:\n",
    "* amplitude, intensity, energy: the sum of values across the frequencies, e.g. the [Root Mean Square](https://librosa.org/doc/0.10.2/generated/librosa.feature.rms.html#librosa.feature.rms) of the signal\n",
    "* entropy: the Shannon entropy of the  $H(x, sf) =  -\\sum_{f=0}^{f_s/2} P(f) \\log_2[P(f)]$\n",
    "* fundamental frequency / pitch: typically computed with an algorithm such as [YIN](https://librosa.org/doc/0.10.2/generated/librosa.yin.html#librosa.yin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938c4c6-1fcb-4614-96fd-f571076004d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound = voc.example('samba.wav', return_type=\"sound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d7eba-54c3-4464-ba00-9d1f68096994",
   "metadata": {},
   "source": [
    "As an example we'll look at some features computed by [Sound Analysis Tools](http://soundanalysispro.com/matlab-sat).\n",
    "\n",
    "The implementation in VocalPy is adapted from work by [Therese Koch](https://therese-koch.netlify.app/) in [AVN](https://github.com/theresekoch/avn/blob/main/avn/acoustics.py) and in [birdsonganalysis](https://github.com/PaulEcoffet/birdsonganalysis) by Paul Ecoffet.\n",
    "\n",
    "We call the function `voc.feature.sat.similarity_features` with a `Sound`, and we get back a set of `vocalpy.Features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00bfa0b-eae7-4c81-8bea-d01a8414c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = voc.feature.sat.similarity_features(sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f9d681-19d1-4acb-8d7a-9a60c644ab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6990240-c967-49eb-805e-40b6993ef095",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_dat.data.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea009734-34d3-4092-93af-8475e58f9729",
   "metadata": {},
   "outputs": [],
   "source": [
    "spect = voc.spectrogram(sound)\n",
    "\n",
    "fig, ax_arr = plt.subplots(3, 1, figsize=(6, 4.5), dpi=150)\n",
    "\n",
    "voc.plot.spectrogram(spect, ax=ax_arr[0])\n",
    "ftr_dat.data['amplitude'].plot(ax=ax_arr[1], color='b')\n",
    "ftr_dat.data['frequency_modulation'].plot(ax=ax_arr[2], color='orange')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499200ed-8f66-4de9-87ff-f5ae62fe9e68",
   "metadata": {},
   "source": [
    "Another use of features is to reduce entire *segments* -- e.g., calls, syllables, etc.--to a 1-dimensional vector of *features*. This gives us a *feature space* we can use with many methods that expect vectors: machine learning classification, dimensionality reduction, distance measures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b1fb6-d20d-435e-a33b-047fc447d69a",
   "metadata": {},
   "source": [
    "## Precomputed Acoustic Features\n",
    "\n",
    "\n",
    "As defined in [Elie Theunissen 2016](https://link.springer.com/article/10.1007/s10071-015-0933-6): a set of extracted features, in contrast with higher dimensionality \"features\". \n",
    "\n",
    "A similar approach is taken by the [warbleR](https://marce10.github.io/warbleR/index.html) function [`spectro_analysis`](https://marce10.github.io/warbleR/reference/spectro_analysis.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41b064-b711-4ea6-a73c-8c712fd6b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_paths = voc.paths.from_dir(\n",
    "    './data/Elie-Theunissen-2016-zebra-finch-song-library-subset/',\n",
    "    'wav'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385f268-a682-410b-85a0-054fb7e4d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, samplerate = librosa.load(wav_paths[0])\n",
    "data = librosa.to_mono(data)\n",
    "sound = voc.Sound(data, samplerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa048164-5b8f-43af-bd7e-eb204c611e98",
   "metadata": {},
   "source": [
    "Here we get the set of features computed by `soundsig`(https://github.com/theunissenlab/soundsig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda494e-3656-44f6-b886-e2a5f1b537cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we skip computing fundamental frequency features since those are slower\n",
    "out = voc.feature.soundsig.predefined_acoustic_features(sound, ftr_groups=(\"temporal\", \"spectral\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf76d91b-6b52-4469-beb7-7a29498055a5",
   "metadata": {},
   "source": [
    "Following the same pattern as before, we will use a `FeatureExtractor` class to extract features from many files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556de577-13bd-42ec-8735-e3129bb0901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = voc.feature.soundsig.predefined_acoustic_features\n",
    "params = dict(ftr_groups=(\"temporal\", \"spectral\"))\n",
    "extractor = voc.FeatureExtractor(callback, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b602d03a-e7f1-4c25-921b-011ea7e36608",
   "metadata": {},
   "outputs": [],
   "source": [
    "sounds = [voc.Sound.read(wav_path) for wav_path in wav_paths[:10]]\n",
    "features_list = extractor.extract(sounds, parallelize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c904a5-b1fd-45b6-b4df-36d243806023",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

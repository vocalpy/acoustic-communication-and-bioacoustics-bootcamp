{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5267f99",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "\n",
    "This notebooks is adapted from [this page](https://vocalpy.readthedocs.io/en/latest/user/howto/segmentation-ir-metrics.html) of the VocalPy docs. It replicates part of the analysis from [Ghaffari Devos 2023](https://dael.euracoustics.org/confs/fa2023/data/articles/000897.pdf), adapting code from https://github.com/houtan-ghaffari/bird_syllable_segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db33941-ccae-462c-a8de-9a9ab04a5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vocalpy as voc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b78d0-2e79-44c9-a379-e80e86754e24",
   "metadata": {},
   "source": [
    "We load an example as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2252b8-5579-4e92-8a3a-f1ae05387d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = voc.example('samba.wav')\n",
    "sound = voc.Sound.read(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ddc365-0bbe-40c6-bcd9-92e4e2995e97",
   "metadata": {},
   "source": [
    "We're going to segment it with one of the methods built into the `vocalpy.segment` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d1d77c-34ab-47b4-9e2b-06f6a422e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = voc.segment.meansquared(sound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70ac7b-e98d-4787-b8b8-41a104051cc3",
   "metadata": {},
   "source": [
    "This gives us back a `vocalpy.Segments` instance, with the starting index and length of a set of line segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d96a95-535a-4e92-9c1b-3454a274fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea3b09-56c6-4517-8fe0-3d84d069005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec558dd",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The goal of our evaluation is to get a measure of how close the segments from an algorithm are to a segmentation created by a human annotator. We will call the annotation created by the human annotator the *ground truth* or *reference* segmentation, and we will call the output of an algorithm the *hypothesis*. \n",
    "\n",
    "The algorithm in question is `vocalpy.segment.meansquared`. This algorithm is used by a Matlab GUI `evsonganaly` originally developed by Evren Tumer in the Brainard Lab, as used in [^3]. The version of the algorithm built into VocalPy is adapted from the Python implementation in the [`evfuncs` package](https://github.com/NickleDave/evfuncs).\n",
    "\n",
    "What we want to understand is the role that different parameters play in the algorithm. \n",
    "To understand the role of these parameters, we will evaluate the output of `vocalpy.segment.meansquared` with and without the clean-up parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105858e5-4643-4d94-8b6b-28bfa8782198",
   "metadata": {},
   "source": [
    "## Baseline algorithm for segmentation\n",
    "\n",
    "We will also compare with another algorithm that simply sets the threshold to the average of the energy, and doesn't do any post-processing to clean up.\n",
    "\n",
    "Adapted from https://github.com/houtan-ghaffari/bird_syllable_segmentation.  \n",
    "See https://dael.euracoustics.org/confs/fa2023/data/articles/000897.pdf for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608ffe6-10bc-4127-b741-ba0202b91ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "import vocalpy as voc\n",
    "\n",
    "\n",
    "def average_envelope_threshold(sound: voc.Sound, cutoff=500, order=40) -> voc.Segments:\n",
    "    \"\"\"Segment audio by threshold with the average of the envelope.\n",
    "\n",
    "    This function (1) high-pass filters the audio to reduce noise,\n",
    "    (2) extracts the Hilbert envelope, (3) smooths the envelope with a Hann window,\n",
    "    and then (4) thresholds the smoothed envelope to segment, \n",
    "    setting the threshold to average of the envelope.\n",
    "    \n",
    "    Adapted from https://github.com/houtan-ghaffari/bird_syllable_segmentation\n",
    "    See https://dael.euracoustics.org/confs/fa2023/data/articles/000897.pdf for more detail.\n",
    "    \"\"\"\n",
    "    if sound.data.shape[0] > 1:\n",
    "        raise ValueError(\n",
    "            f\"The ``sound`` has {sound.data.shape[0]} channels, but segmentation is not implemented \"\n",
    "            \"for sounds with multiple channels. This is because there can be a different number of segments \"\n",
    "            \"per channel, which cannot be represented as a rectangular array. To segment each channel, \"\n",
    "            \"first split the channels into separate ``vocalpy.Sound`` instances, then pass each to this function.\"\n",
    "            \"For example,\\n\"\n",
    "            \">>> sound_channels = [sound_ for sound_ in sound]  # split with a list comprehension\\n\"\n",
    "            \">>> channel_segments = [vocalpy.segment.meansquared(sound_) for sound_ in sound_channels]\\n\"\n",
    "        )\n",
    "\n",
    "    x = sound.data.squeeze(axis=0)\n",
    "\n",
    "    sos = scipy.signal.butter(\n",
    "        order, cutoff, btype=\"highpass\", analog=False, output=\"sos\", fs=sound.samplerate\n",
    "    )\n",
    "    x = scipy.signal.sosfiltfilt(sos, x)\n",
    "    x = np.abs(scipy.signal.hilbert(x))\n",
    "    win = scipy.signal.windows.hann(512)\n",
    "    x = scipy.signal.convolve(x, win, mode='same') / sum(win)\n",
    "    threshold = np.mean(x)\n",
    "    above_threshold = x > threshold\n",
    "\n",
    "    # convolving with h causes:\n",
    "    # +1 whenever above_th changes from 0 to 1\n",
    "    # and -1 whenever above_th changes from 1 to 0\n",
    "    h = np.array([1, -1])\n",
    "    above_convolved = np.convolve(h, above_threshold)\n",
    "    onsets_sample = np.where(above_convolved > 0)[0]\n",
    "    offsets_sample = np.where(above_convolved < 0)[0]\n",
    "    lengths = offsets_sample - onsets_sample\n",
    "\n",
    "    return voc.Segments(        \n",
    "        start_inds=onsets_sample,\n",
    "        lengths=lengths,\n",
    "        sound=sound\n",
    "    )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a02579",
   "metadata": {},
   "source": [
    "### Information retrieval metrics\n",
    "\n",
    "The metrics we will use to evaluate segmentation are adapted from the field of [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval). Broadly speaking, this field builds systems to retrieve information, e.g. a program that lets a user query a database of documents (think Google Search). Methods for evaluating these systems have been borrowed by related fields, and by machine learning more broadly. For example, one can also conceive of querying a dataset of audio, as is done in [music information retrieval](https://musicinformationretrieval.com/), and information retrieval metrics have been adapted to evaluate the segentation of audio. See for example the segmentation metrics in the [`mir_eval`](https://craffel.github.io/mir_eval/#module-mir_eval.segment) package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68b5ab",
   "metadata": {},
   "source": [
    "## Compare segmentation algorithms\n",
    "\n",
    "### Get data\n",
    "\n",
    "We will use a subset of data from the [Bengalese Finch Song Repository](https://nickledave.github.io/bfsongrepo/), that is built into VocalPy as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb10e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfsongrepo_dir = './data/Nicholson-Queen-Sober-2017-bfsongrepo-subset/'\n",
    "wav_paths = voc.paths.from_dir(bfsongrepo_dir, 'wav')\n",
    "csv_paths = voc.paths.from_dir(bfsongrepo_dir, 'csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa577fd",
   "metadata": {},
   "source": [
    "### Segment audio\n",
    "\n",
    "To set ourselves up to analyze the results below, we will make a Python dictionary that maps a string `'name'` (for the algorithm we are testing) to the {py:class}`list` of {py:class}`vocalpy.Segments` returned by the algorithm. Since we test the same algorithm twice with different parameters, we will think of this name as a \"condition\" (the three conditions we outlined above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed33cca-7f53-463a-b5f8-632faade0920",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_segments_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4356b0ba-41c1-4f66-a306-63fbfd287e0f",
   "metadata": {},
   "source": [
    "To actually get the results, we will write a loop.\n",
    "Inside the loop, we will use the {py:class}`vocalpy.Segmenter` class.  \n",
    "This class takes has two parameters: a `callback` function, and the `params` (parameters) we will pass to that function.\n",
    "Each time through the loop, we will make an instance of the `Segmenter` class by passing in a specific `callback` and set of `params` as the two arguments. Then when we call the `segment` method on that instance, the class will call the `callback` function.\n",
    "So, at the top of the loop, we define a tuple of 3-element tuples that we iterate through. Each 3-element tuple has the condition `name`, the `callback` and the `params` we will use with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df868e35-fa1c-4735-8d41-911a7cf64791",
   "metadata": {},
   "outputs": [],
   "source": [
    "sounds = [\n",
    "    voc.Sound.read(wav_path)\n",
    "    for wav_path in wav_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a24fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, callback, params in (\n",
    "    ('meansquared', voc.segment.meansquared, voc.segment.MeanSquaredParams(threshold=1500, min_dur=0.01, min_silent_dur=0.006)),\n",
    "    # here we set the parameters for minimum durations to zero, meaning \"don't filter\"\n",
    "    ('meansquared-no-cleanup', voc.segment.meansquared, voc.segment.MeanSquaredParams(threshold=1500, min_dur=0., min_silent_dur=0.)),\n",
    "    # our baseline. We set the params to None. We write it here so we have a value for \"params\" when we loop through these\n",
    "    # even though the default is None\n",
    "    ('average_envelope_threshold', average_envelope_threshold, None)\n",
    "):\n",
    "    print(f\"Segmenting with algorithm/condition: '{name}'\")\n",
    "    segmenter = voc.Segmenter(callback, params)\n",
    "    algo_segments_map[name] = segmenter.segment(sounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b034b714",
   "metadata": {},
   "source": [
    "### Get ground truth data\n",
    "\n",
    "Now we need our reference segmentation to compare to.\n",
    "\n",
    "We will need list of {py:class}`vocalpy.Segments` for this too, but here we make them using {py:class}`~vocalpy.Annotation` that we load from the example datasets. These annotations contain the ground truth segmentation that we want to compare with the results from the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b2ed1f-0106-4fa9-8e2b-b879f83bed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "annots = [\n",
    "    voc.Annotation.read(csv_path, format='simple-seq')\n",
    "    for csv_path in csv_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe002da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_segments_list = []\n",
    "for annot, sound in zip(annots, sounds):\n",
    "    start_inds = (annot.data.seq.onsets_s * sound.samplerate).astype(int)\n",
    "    stop_inds = (annot.data.seq.offsets_s * sound.samplerate).astype(int)\n",
    "    lengths = stop_inds - start_inds\n",
    "    ref_segments_list.append(\n",
    "        voc.Segments(\n",
    "            start_inds,\n",
    "            lengths,\n",
    "            sound=sound,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6b70de",
   "metadata": {},
   "source": [
    "### Compute metrics\n",
    "\n",
    "Finally we use the functions in the {py:module}`vocalpy.metrics.segmentation.ir` module to compute metrics that we use to compare the segmentation algorithms.\n",
    "\n",
    "The functions in this module expect two arguments: a `reference` segmentation, and a `hypothesis`. In other words, the ground truth and the output of some algorithm that we want to compare with that ground truth.\n",
    "\n",
    "Notice that we use a `tolerance` of 10 milliseconds. This is a standard value used in previous work--you may find it instructive to vary this value and examine the results.\n",
    "\n",
    "We will make a {py:class}`pandas.DataFrame` with the results, that we will then plot with {py:module}`seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d75fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_records = []  # will become a DataFrame\n",
    "\n",
    "\n",
    "TOLERANCE = 0.01  # milliseconds\n",
    "\n",
    "\n",
    "for name, hypothesis_segments_list in algo_segments_map.items():\n",
    "    for reference, hypothesis in zip(ref_segments_list, hypothesis_segments_list):\n",
    "        prec, _, _ = voc.metrics.segmentation.ir.precision(\n",
    "            reference=reference.all_times,\n",
    "            hypothesis=hypothesis.all_times,\n",
    "            tolerance=0.01  # 10 milliseconds\n",
    "        )\n",
    "        rec, _, _ = voc.metrics.segmentation.ir.recall(\n",
    "            reference=reference.all_times,\n",
    "            hypothesis=hypothesis.all_times,\n",
    "            tolerance=0.01  # 10 milliseconds\n",
    "        )\n",
    "        fscore, _, _ = voc.metrics.segmentation.ir.fscore(\n",
    "            reference=reference.all_times,\n",
    "            hypothesis=hypothesis.all_times,\n",
    "            tolerance=TOLERANCE\n",
    "        )\n",
    "        for metric_name, metric_val in zip(\n",
    "            ('precision', 'recall', 'fscore'),\n",
    "            (prec, rec, fscore)\n",
    "        ):\n",
    "            results_records.append(\n",
    "                {\n",
    "                    'condition': name,\n",
    "                    'metric': metric_name,\n",
    "                    'value': metric_val,\n",
    "                }\n",
    "            )\n",
    "\n",
    "results_df = pd.DataFrame.from_records(results_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeefd97",
   "metadata": {},
   "source": [
    "We inspect the dataframe to check that it looks like what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5814c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe8481",
   "metadata": {},
   "source": [
    "Note this is in \"long\" form where we have a \"variable\" column -- in this case, the different metrics -- and the value that each \"variable\" takes on.\n",
    "\n",
    "We could alternatively have the `DataFrame` in wide form, with columns for `'precision'`, `'recall'`, and `'fscore`, but as we'll see, the long form makes it easier to plot below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44688b",
   "metadata": {},
   "source": [
    "### Visualize results\n",
    "\n",
    "Now let's do some final clean-up of the dataframe, for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d407133",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['value'] = results_df['value'] * 100.\n",
    "\n",
    "results_df['metric'] = results_df['metric'].map(\n",
    "    {\n",
    "        'fscore': '$F$-score (%)',\n",
    "        'precision': 'Precision (%)',\n",
    "        'recall': 'Recall (%)',\n",
    "    },\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1477abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_context('talk', font_scale=1.25)\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "\n",
    "g = sns.catplot(\n",
    "    results_df,\n",
    "    y='value',\n",
    "    hue='condition',\n",
    "    col='metric',\n",
    "    kind='bar',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d37f4",
   "metadata": {},
   "source": [
    "We can see that the `'meansquared'` algorithm *with* the clean-up steps has the highest precision and recall. Perhaps surprisingly, the `'meansquared'` algorithm *without* clean-up has a *lower* precision than the `average_envelope_threshold`, and even more surprisingly, it has the highest recall of all. We can understand this as follows: the `'meansquared'` algorithm finds \"more\" segments than the `'average_envelope_threshold'` algorithm--giving it a higher recall--but that also means it returns more false positives. More generally, a result like this would suggest that the clean-up steps have a bigger impact on performance than the methods used to compute the energy and the exact threshold used. Keep in mind that we've shown here is just a demo. To really draw this conclusion we'd need to do an extensive analysis across datasets, and be very clear about our intended use cases for the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c0a113",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[^1]: Kershenbaum, Arik, et al.\n",
    "\"Acoustic sequences in non‐human animals: a tutorial review and prospectus.\" \n",
    "Biological Reviews 91.1 (2016): 13-52.\n",
    "https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12160\n",
    "\n",
    "[^2]: Odom, Karan J., et al. \n",
    "\"Comparative bioacoustics: a roadmap for quantifying and comparing animal sounds across diverse taxa.\" \n",
    "Biological Reviews 96.4 (2021): 1135-1159.\n",
    "https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12695\n",
    "\n",
    "[^3]: Tumer, Evren C., and Michael S. Brainard.\n",
    "\"Performance variability enables adaptive plasticity of ‘crystallized’ adult birdsong.\"\n",
    "Nature 450.7173 (2007): 1240-1244.\n",
    "https://www.academia.edu/download/46524972/nature06390.pdf\n",
    "\n",
    "[^4]: Kemp, T., Schmidt, M., Whypphal, M., & Waibel, A. (2000, June).\n",
    "Strategies for automatic segmentation of audio data.\n",
    "In 2000 ieee international conference on acoustics, speech, and signal processing.\n",
    "proceedings (cat. no. 00ch37100) (Vol. 3, pp. 1423-1426). IEEE.\n",
    "http://www1.icsi.berkeley.edu/~dpwe/research/etc/icassp2000/pdf/980_1114.PDF\n",
    "\n",
    "[^5]: Jordán, P. G., & Giménez, A. O. (2023).\n",
    "Advances in Binary and Multiclass Sound Segmentation with Deep Learning Techniques.\n",
    "https://www.researchgate.net/profile/Pablo-Gimeno/publication/371567111_Advances_in_Binary_and_Multiclass_Audio_Segmentation_with_Deep_Learning_Techniques/links/648a2fba7fcc811dcdce3d3c/Advances-in-Binary-and-Multiclass-Audio-Segmentation-with-Deep-Learning-Techniques.pdf\n",
    "\n",
    "[^6]: Ghaffari, Houtan, and Paul Devos.\n",
    "\"Consistent Birdsong Syllable Segmentation Using Deep Semi-Supervised Learning.\" (2023).\n",
    "https://dael.euracoustics.org/confs/fa2023/data/articles/000897.pdf\n",
    "https://github.com/houtan-ghaffari/bird_syllable_segmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659d9949-4b5a-485f-892c-5dad43dc4c3b",
   "metadata": {},
   "source": [
    "# Individually identifying songs of Great Tits (*Parus major*)\n",
    "\n",
    "The Great Tit (*Parus major*) is known for its varied song repertoire. Seventy song types are known, with each individual's repertoire including up to eight song types. \n",
    "\n",
    "<center><img src=\"https://i.guim.co.uk/img/media/b98326a736e0c4e5d88846102bef16414b6450a5/0_0_4960_2976/master/4960.jpg?width=1200&height=900&quality=85&auto=format&fit=crop&s=b693c659d0b233d394e2fa3e28863701\" alt=\"Great Tit (Parus major) singing\" width=\"300\" /></center>\n",
    "\n",
    "In this exercise we will be working with the [Wytham Great Tit Song Dataset](https://nilomr.github.io/great-tit-hits/), which includes a large amount of Great Tit songs recorded in the wild. Each burst of song is identified by the individual that sung it as well as by the song type.\n",
    "\n",
    "We will inspect a handful of examples from each individual, with all examples belonging to the same song type for that individual. Using a pretrained convolutional neural network, we will generate feature embeddings for the spectrogram of each song, then use t-SNE to cluster these embeddings. We will see that the convolutional neural network picks up on enough distinctive features of each song to allow us to cluster them well by individual.\n",
    "\n",
    "## Import packages\n",
    "\n",
    "This notebook should be run in an environment containing OpenSoundscape (see instructions for your system [here](https://opensoundscape.readthedocs.io/en/latest/index.html)) and `tensorflow-cpu`.\n",
    "\n",
    "Alternatively, you may install them on a Google Colab notebook using the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae77c2-1a2a-4fc5-9953-733f905ec711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this is a Google Colab notebook, install opensoundscape in the runtime environment\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  %pip install opensoundscape==0.10.1\n",
    "  %pip install tensorflow-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be122add-f45a-4beb-9297-3339b58439fe",
   "metadata": {},
   "source": [
    "The following cell make take a while...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db53c14-b13c-454c-a30d-cf398835e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from opensoundscape.audio import Audio\n",
    "from opensoundscape.spectrogram import Spectrogram\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9be8fd-ea31-43cb-8d57-568238a515a2",
   "metadata": {},
   "source": [
    "## Download files\n",
    "\n",
    "These files are located on our Google Drive folder here: https://drive.google.com/drive/u/0/folders/1-XOEcKHOSiDSIlSHElOeJY5Jza4Hc-7v but you likely already downloaded them if you followed the steps in the README.\n",
    "\n",
    "They should be downloaded and placed in the `data` folder of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659ca1c-7083-46a2-b8dc-594b7ff6a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_folder = \"../data/Merino-Recalde-Wytham-Great-Tit-Dataset-2023-subset/wytham_songs/\"\n",
    "csv_path = \"../data/Merino-Recalde-Wytham-Great-Tit-Dataset-2023-subset/wytham_annotations.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e33ff-10ec-41be-84ba-b669f2903711",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Start with a list of wavfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90531635-6815-4a5e-b34c-37bff35faca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wavfiles from subset of the Wytham Great Tit dataset\n",
    "wavfiles = list(Path(wav_folder).glob(\"*.wav\"))\n",
    "wavfiles = sorted(wavfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152ff73-5d26-42ef-9774-0ed1613c6db6",
   "metadata": {},
   "source": [
    "Now, get the annotations for these wavfiles and check out what this dataframe looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d1112-5661-404e-8d12-e6b88138ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "annots_df = pd.read_csv(csv_path, index_col=0)\n",
    "annots_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ad89fd-099f-48df-9dd3-ea40f4f47310",
   "metadata": {},
   "source": [
    "The `ID` column contains individual IDs. How many individual IDs are in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec35b49-c3a9-4bbb-b5e6-6b0f7d1a8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of individuals:\", len(annots_df.ID.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff0914-cfbc-4f3d-b08e-7c3c56f98ead",
   "metadata": {},
   "source": [
    "The `class_id` contains the class of the song. Note that this subset of data was specifically chosen to only have one class_id per individual (see the end of this notebook for more discussion on that)!\n",
    "\n",
    "If you download the original Great Tit dataset (https://osf.io/n8ac9/) and want to reproduce this subsetting, only looking at songs from an individual that are of the same song type as each other, you could use the following code to produce a dataframe of the most common song type per individual:\n",
    "```\n",
    "subset_df = annots_df.value_counts([\"ID\", \"class_id\"]).reset_index().drop_duplicates(\"ID\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80fa682-7070-4766-be0e-6e6607618d2a",
   "metadata": {},
   "source": [
    "Let's look at a subsample of songs from the 20 most common song types in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef705a6-65e7-459b-a19d-a7ced5d64750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 20 most common song types\n",
    "top_song_types = annots_df.value_counts([\"class_id\"]).reset_index()[:20]\n",
    "\n",
    "# Get all annotations for these songtypes\n",
    "all_annots_songtypes = annots_df[annots_df.class_id.isin(top_song_types.class_id.values)]\n",
    "\n",
    "# Randomly sample 20 recordings per song type\"\n",
    "test_songs = all_annots_songtypes.groupby(\"class_id\").sample(30, random_state=3)\n",
    "\n",
    "# Make sure there are 20 per song type\n",
    "test_songs.class_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d5b6d-8de2-4853-8444-a282d76490e8",
   "metadata": {},
   "source": [
    "Let's visualize an example using OpenSoundscape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70470c15-c431-4b39-ba9a-b9163ddea3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single song ID\n",
    "idx = 1\n",
    "song_id = test_songs.index[idx]\n",
    "annots = test_songs.loc[song_id]\n",
    "wavfile = Path(wav_folder + song_id + \".wav\")\n",
    "\n",
    "# Load the audio of this song\n",
    "individ_audio = Audio.from_file(wavfile)\n",
    "\n",
    "# View and listen to the song\n",
    "Spectrogram.from_audio(individ_audio).plot()\n",
    "individ_audio.show_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ce4b66-5e66-4655-b45e-aca7dc280ede",
   "metadata": {},
   "source": [
    "There's a lot of noise in the recording. One way we could explore noise reduction is to bandpass the file with the annotated frequency limits..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e18cc5-af87-495f-ac04-d9ddef3a5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "individ_audio = Audio.from_file(wavfile).bandpass(annots.lower_freq, annots.upper_freq, order=4)\n",
    "Spectrogram.from_audio(individ_audio).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1d623d-982e-4054-822f-8b00a7e3f063",
   "metadata": {},
   "source": [
    "But for simplicity, in this notebook, we will just use the raw audio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c61fa-13ec-4f69-bf8b-7c6087e238aa",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cc894e-44f9-402a-8f9e-15b808f76a84",
   "metadata": {},
   "source": [
    "Now let's load a model we can use to generate unsupervised feature embeddings for recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bbfab1-c72c-4c80-b0a1-a90d6724acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "m = torch.hub.load('kitzeslab/bioacoustics-model-zoo', 'BirdNET',trust_repo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74768315-2ec1-4fa3-a9ea-67d4e162f497",
   "metadata": {},
   "source": [
    "In order to use this model, we need to create a list of files in a pandas DataFrame. Each row supplies one 3s long clip.\n",
    "* `filename` of each recording to predict on\n",
    "* `start_time` and `end_time` within each filename.\n",
    "    * Generally, clips should be 3s long for this model.\n",
    "    * If you are working with long audio files and want to generate embeddings from the whole audio file, you will want to include many rows for one `filename` each with a different `start_time` and `end_time`\n",
    "    * You can generate embeddings with overlap (e.g. for a clip from 0s to 3s, a clip from 1s to 4s, a clip from 2s to 5s, etc.) by creating rows for this.\n",
    " \n",
    "Note that many of the clips in this dataset are not near 3s long, so this is just a first approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773dcf08-cde1-4e3f-93a9-cfab805b664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wavfiles = [wav_folder + song_id + \".wav\" for song_id in test_songs.index]\n",
    "\n",
    "# Create a formatted dataframe as required by OpenSoundscape\n",
    "wavfiles_df = pd.DataFrame(test_wavfiles)\n",
    "wavfiles_df.columns = [\"file\"]\n",
    "wavfiles_df[\"start_time\"] = 0\n",
    "wavfiles_df[\"end_time\"] = 3\n",
    "\n",
    "# Need to set these column names as the index for OpenSoundscape\n",
    "wavfiles_df = wavfiles_df.set_index([\"file\", \"start_time\", \"end_time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2bb62-13d0-4cff-81c6-bad7da93dc9b",
   "metadata": {},
   "source": [
    "Use the model to generate embeddings for all of the recordings. \n",
    "\n",
    "This cell will take a little while to run; on the author's system, it took about 40 seconds to run (6-7 seconds per 100 files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336b9da-e3f6-43a5-ade5-55f62cefbe12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture --no-stdout\n",
    "# This will raise many warnings due to the differing \n",
    "# lengths of the wavfiles, so in this cell we catch all output\n",
    "embeddings = m.generate_embeddings(wavfiles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d84346c-7682-4bea-9d61-1ab7adc07419",
   "metadata": {},
   "source": [
    "Use t-SNE to dimensionally reduce the embeddings. t-SNE is a method for dimensionally reducing high-dimensional data. It is a stochastic, iterative process. Parameters you can modify:\n",
    "* You can change the `random_state` below to initialize t-SNE at a different random starting point.\n",
    "* You can also increase the number of iterations (`n_iter`), which generally improves the tightness of clusters.\n",
    "* You can change the `perplexity`, but since we know there are 30 datapoints per class, 30 is probably a good number. In your own applications, if you're not sure how many datapoints there are per class, you can try different numbers to see which produces the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c719908-6f15-4ee4-9a19-4e1a81698443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=5000)\n",
    "embeddings_tsne = tsne.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe8fb7-808e-4e6b-ad80-cb609b0b05dc",
   "metadata": {},
   "source": [
    "Plot the dimensionally reduced embeddings, colored by the ID of each individual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15045843-38d0-40cd-997b-1bd08a4d3a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "individual_ids = test_songs.ID\n",
    "color_map = iter(cm.viridis(np.linspace(0, 1, len(individual_ids.unique()))))\n",
    "for i, individual_idx in enumerate(np.unique(individual_ids)):\n",
    "    color = next(color_map)\n",
    "    embeddings_individual = embeddings_tsne[individual_ids == individual_idx]\n",
    "    ax.scatter(embeddings_individual[:, 0], y=embeddings_individual[:, 1], label=individual_idx, color=color, alpha=0.5)\n",
    "ax.legend(ncols=2, bbox_to_anchor=(1, 1), title=\"Individual ID\")\n",
    "plt.title(\"t-SNE dimensionality reduction of embeddings of\\n songs of individual Great Tits (Parus major)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dfca53-2cc1-4c2b-8021-68503885fcb1",
   "metadata": {},
   "source": [
    "This looks fantastic, though not perfect!\n",
    "\n",
    "Let's see what each of these individual's songs actually look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff2834-5fb2-4cad-a583-c6efd83ed608",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_plot_per_individual = 4\n",
    "fig, axs = plt.subplots(20, songs_plot_per_individual, figsize=(10,20))\n",
    "for individual_idx, (individual_id, df) in enumerate(test_songs.groupby([\"ID\"])):\n",
    "    for song_idx, song_id in enumerate(df.index[:songs_plot_per_individual]):\n",
    "        # Get the axis in the correct position\n",
    "        _ = plt.sca(axs[individual_idx, song_idx])\n",
    "        ax = plt.gca()\n",
    "        \n",
    "        # Get the spectrogram and plot it\n",
    "        s = Spectrogram.from_audio(Audio.from_file(wav_folder + song_id + \".wav\")).bandpass(2000,7000).spectrogram\n",
    "        _ = plt.imshow(s, aspect=0.3, cmap=\"Greys\")\n",
    "        ax.invert_yaxis() # When using imshow, have to invert this\n",
    "        \n",
    "        # Nice formatting\n",
    "        # Hide X and Y axes label marks\n",
    "        ax.xaxis.set_tick_params(labelbottom=False)\n",
    "        ax.yaxis.set_tick_params(labelleft=False)\n",
    "        # Hide X and Y axes tick marks\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if song_idx == 0:\n",
    "            ax.set_title(individual_id[0])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809985f9-307d-42a5-a848-a8c55f29dcf3",
   "metadata": {},
   "source": [
    "# Potential next steps\n",
    "\n",
    "## Improving clustering\n",
    "\n",
    "One of the biggest challenges of clustering the sounds of wild animals is that other noises overlapping the sound can affect clustering performance. Some options for noise reduction include:\n",
    "* Modifying the pipeline to bandpass the recordings to the annotated range\n",
    "* Applying noise reduction using the Python `noisereduce` package (doesn't work as well on short recordings) or the [Google MixIT bird separation model](https://github.com/kitzeslab/bioacoustics-model-zoo?tab=readme-ov-file#mixit-bird-separationmodel) implemented in OpenSoundscape\n",
    "\n",
    "## Inspecting similar song types across individuals\n",
    "\n",
    "The subset of recordings includes the song most commonly sung for 42 individuals in \"part 1\" of the Great Tit dataset. Over 70 different song types are known, with an individual singing up to eight different song types. While \"song types\" are each very different from each other, two different individuals singing the same song type might actually sound quite similar.\n",
    "\n",
    "A future approach could be to select a song type that many individuals use, and see if individuals can still be identified by the particular quirks in how they sing that song type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c11138-c646-495e-9ec9-5c9457519c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a7a820c-f9f0-4047-bcb5-09b450222bf1",
   "metadata": {},
   "source": [
    "# Working with acoustic communication data in VocalPy\n",
    "\n",
    "This tutorial introduces us to VocalPy, that we'll use in the other notebooks.\n",
    "\n",
    "It is adapted from https://vocalpy.readthedocs.io/en/latest/getting_started/quickstart.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7cc563-6fb3-44e5-a766-cf603a007663",
   "metadata": {},
   "source": [
    "### Let's start with some example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050d588-8ecc-426f-a0f0-844616a14e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vocalpy as voc\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df0927-64d0-44a1-90aa-949fc82600e2",
   "metadata": {},
   "source": [
    "VocalPy has built-in examples of acoustic communication data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e8bc90-3554-42ca-a5eb-03d7f8aa0000",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc.examples.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da425ed1-0dc1-46bf-9424-492294ebaeb3",
   "metadata": {},
   "source": [
    "If we just give it the name of an example, we get back a path to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456d343-53b7-4995-8ec2-cdfeef463ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = voc.example('samba.wav')\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1291a0-f9b1-4dd6-83fa-5111ce8d7e03",
   "metadata": {},
   "source": [
    "## Data types for acoustic communication\n",
    "\n",
    "We'll use the example data to look at the data types that VocalPy provides for acoustic comunication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b8d42-1598-455a-ac63-33d1a355f5ac",
   "metadata": {},
   "source": [
    "### Data type for sound: `vocalpy.Sound`\n",
    "\n",
    "We read a sound in from a file with the `Sound.read` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f615d-359e-4b9f-b8af-6a1176b97ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound = voc.Sound.read(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f49204-5333-47cd-a30d-48eb27721583",
   "metadata": {},
   "source": [
    "We now have a data container with the sound `data` itself as well as the `samplerate` as an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632217bd-1179-4e1b-bae5-ca35d0dfd72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa618ba-b5d6-4459-90ca-a7294a9a566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea188c-2ce9-4874-932c-bfb858179ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound.samplerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf635c-828c-423a-9e0f-0347968f8649",
   "metadata": {},
   "source": [
    "A `Sound` also has three properties, derived from its data:\n",
    "\n",
    "1. channels, the number of channels\n",
    "\n",
    "2. samples, the number of samples, and\n",
    "\n",
    "3. duration, the number of samples divided by the sampling rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0feb9f-6689-438d-955d-15c4fa3f6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"This sound comes from an audio file with {sound.channels} channel, \"\n",
    "    f\"{sound.samples} samples, and a duration of {sound.duration:.3f} seconds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a9d84-d890-4588-aa13-7128c4c8be24",
   "metadata": {},
   "source": [
    "One of the reasons VocalPy provides this data type, and the others we’re about to show you here, is that it helps you write more succinct code that’s easier to read: for you, when you come back to your code months from now, and for others that want to read the code you share.\n",
    "\n",
    "The properties and attributes let us avoid writing a bunch of variable names like `data` and `fs` and `samplerate` that clutter up our code, when what we really want to think about are the sound itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c355f-8f8a-4189-96e3-70e5ce72400f",
   "metadata": {},
   "source": [
    "Let's visually inspect the sound. Notice that if we need the data as a NumPy array, we can use the `data` attribute. This lets VocalPy work well with other Python packages, such as `librosa` (https://librosa.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0afa078-09b8-4bd5-995d-118ef3d5aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.waveshow(sound.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33d171-efab-40cf-b87e-663b129f126e",
   "metadata": {},
   "source": [
    "We can see some peaks suggesting activity, but we can't tell much else about this sound just by looking at it as a time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8215e6f-46db-43ef-839a-5e6ab5079ea4",
   "metadata": {},
   "source": [
    "## Data type: `vocalpy.Spectrogram`\n",
    "\n",
    "* Intuitively: a picture of sound\n",
    "* Technically speaking: \"the squared magnitude of the Short Time Fourier Transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3debccc8-9bda-44ad-83fd-7e962e1b6348",
   "metadata": {},
   "outputs": [],
   "source": [
    "spect = voc.spectrogram(sound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5470113c-b2d2-4179-9c74-3bc643002ce6",
   "metadata": {},
   "source": [
    "As before, we’ll walk through the attributes of this class. But since the whole point of a spectrogram is to let us see sound, let’s actually look at the spectrogram, instead of staring at arrays of numbers.  \n",
    "\n",
    "We do so by calling `vocalpy.plot.spectrogram`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a7e7d-93fe-4505-9ad4-a7514e611e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc.plot.spectrogram(spect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc088ac8-3d40-4eee-aa91-053c4ec0f589",
   "metadata": {},
   "source": [
    "Now we can see that we are working with zebra finch song!  \n",
    "\n",
    "Now that we know what we’re working with, let’s actually inspect the attributes of the vocalpy.Spectrogram instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10adda41-378a-4488-9695-364b5001ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "spect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550724d7-e04d-40f5-b440-57b9abb31a60",
   "metadata": {},
   "source": [
    "There are five attributes we care about here.\n",
    "\n",
    "1. `data`: this is the spectrogram itself – as with the other data types, like `vocalpy.Sound`, the attribute name `data` indiciates this main data we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f303c-4284-45bc-aee5-ad4361ea350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spect.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ddaf49-517c-4807-b968-9b5192a78d05",
   "metadata": {},
   "source": [
    "Let’s look at the shape of `data`. It’s really just a NumPy array, so we inspect the array’s shape attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d8379-d74a-4f14-9be9-26b3f0306269",
   "metadata": {},
   "outputs": [],
   "source": [
    "spect.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824fbb21-0dd5-422e-8071-95463c68de75",
   "metadata": {},
   "source": [
    "We see that we have an array with dimensions (channels, frequencies, times). The last two dimensions correspond to the next two attributes we will look at.\n",
    "\n",
    "2. `frequencies`, a vector of the frequency for each row of the spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31aa4d0-68b8-4f16-8315-a059467aa55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spect.frequencies[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b80bfe9-07bf-410a-9698-41b2eef9511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spect.frequencies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b74d91-c7df-480c-a389-3e395ee6886b",
   "metadata": {},
   "source": [
    "(We see it is equal to the number of elements in the second dimension of `data`.)\n",
    "\n",
    "3. `times`, a vector of the time for each column in the spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16226e9e-998b-4b54-861f-79af38583d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spect.times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff6c6c7-a037-4158-95b4-d811140c6f3c",
   "metadata": {},
   "source": [
    "Just like with the `Sound` class, VocalPy gives us the ability to conveniently read and write spectrograms from files. This saves us from generating spectrograms over and over. Computing spectrograms can be computionally expensive, if your audio has a high sampling rate or you are using methods like multi-taper spectrograms. Saving spectrograms from files also makes it easier for you to share your data in the exact form you used it, so that it’s easier to replicate your analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfe490-d07b-415e-a094-51c0dfcaa211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "DATA_DIR = pathlib.Path('./data/my-data')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "spect_file = spect.write(DATA_DIR / f\"{spect.audio_path.name}.spect.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fcd95d-95e1-45be-8dad-36a665c08569",
   "metadata": {},
   "source": [
    "Notice that the extension is `'npz'`; this is a file format that NumPy uses to save mulitple arrays in a single file: We are saving the `data`, the `frequencies` and the `times`.\n",
    "\n",
    "By convention we include the file extension of the source audio, and another “extension” that incidicates this is a spectrogram, so that the file name ends with `'.wav.spect.npz'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7258b2f-a88a-449f-b385-69f089acc72c",
   "metadata": {},
   "source": [
    "## Classes for steps in pipelines for processing data in acoustic communication\n",
    "\n",
    "In addition to data types for acoustic communication, VocalPy provides you with classes that represent steps in pipelines for processing that data. These classes are also written with readability and reproducibility in mind.\n",
    "\n",
    "Let's say we want to make spectrograms for all of our sound data. We'll use one of the classes, `SpectrogramMaker`, to make a spectrogram from each one of a set of wav files.\n",
    "\n",
    "When you are working with your own data, instead of example data built into VocalPy, you will do something like:\n",
    "\n",
    "1. Load all the sound files from a directory using a convenience function that VocalPy gives us in its paths module, `vocalpy.paths.from_dir`\n",
    "\n",
    "2. Load all the wav files into the data type that VocalPy provides for sound, `vocalpy.Sound`, using the method `vocalpy.Sound.read`\n",
    "\n",
    "This is shown in the snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ab44f-818e-435e-8494-1341d7d7f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfsongrepo_dir = './data/Nicholson-Queen-Sober-2017-bfsongrepo-subset/'\n",
    "wav_paths = voc.paths.from_dir(dir=bfsongrepo_dir, ext='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac692911-b64c-45dc-9376-08877d2b09ce",
   "metadata": {},
   "source": [
    "Read in all the `Sound`s with a list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae794cac-aec0-4c3a-9d6b-d8a55d26bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sounds = [\n",
    "    voc.Sound.read(wav_path)\n",
    "    for wav_path in wav_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aae0f2-717c-4e43-890e-e8df88790a43",
   "metadata": {},
   "source": [
    "Now we use the `SpectrogramMaker` class to make spectrograms from all these `Sound`s.\n",
    "\n",
    "To make sure our code is readable to \"future us\", we write down the function we used as a `callback`, and we write down the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be5b553-1a1f-45fa-a1bf-45a3da59d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function we used\n",
    "callback = voc.spectrogram\n",
    "# these are the parameters we used\n",
    "params = dict(n_fft=512, hop_length=64)\n",
    "spect_maker = voc.SpectrogramMaker(\n",
    "    callback, params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c202f-48fc-4be0-b1e2-1243d3fa07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spect_maker.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7485cb19-3a8e-4aa8-9971-ebd61374b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "spects = spect_maker.make(sounds, parallelize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26cff5-8606-474d-908f-a6d99f387431",
   "metadata": {},
   "source": [
    "Let's plot four of the spectrograms we made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db052339-145f-47c0-9ca3-2500bca10137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax_arr = plt.subplots(2, 2, figsize=(10, 5))\n",
    "ax_arr = ax_arr.ravel()\n",
    "\n",
    "for ax, spect in zip(ax_arr, spects):\n",
    "    voc.plot.spectrogram(spect, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8567d928-7a34-4195-bfab-86f794cbb7a9",
   "metadata": {},
   "source": [
    "## Data type: `vocalpy.Annotation`\n",
    "\n",
    "The last data type we’ll look at is for annotations. Such annotations are important for analysis of aocustic communication and behavior. Under the hood, VocalPy uses the pyOpenSci package `crowsetta`(https://github.com/vocalpy/crowsetta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd607db5-3216-4ac6-8ec3-6963ff9abfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = voc.paths.from_dir('data/Nicholson-Queen-Sober-2017-bfsongrepo-subset/', 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d1c0e-c109-456e-b0ea-c37e0d7038e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cc7655-9b07-4a2c-8d3d-851a877800df",
   "metadata": {},
   "outputs": [],
   "source": [
    "annots = [\n",
    "    voc.Annotation.read(csv_path, format='simple-seq')\n",
    "    for csv_path in csv_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a43228b-9bb2-41cc-91b6-c95acb3cbc27",
   "metadata": {},
   "source": [
    "We inspect one of the annotations. Again as with other data types, we can see there is a `data` attribute. In this case it contains the `crowsetta.Annotation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c2cf1-ff33-4811-889f-0b53bff2c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(annots[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051d0ef7-193f-4988-8435-8c206cfce72d",
   "metadata": {},
   "source": [
    "We plot the spectrogram along with the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97586908-04f9-479c-bab0-fd2bb69bb1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc.plot.annotated_spectrogram(spects[0], annots[0], tlim=[1.8, 4]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6250c-aadd-4b99-9532-00c3b7968c18",
   "metadata": {},
   "source": [
    "This crash course in VocalPy has introduced you to the key features and goals of the library. To learn more, please check out the [documentation](https://vocalpy.readthedocs.io/en/latest/), read our Forum Acusticum 2023 Proceedings Paper, [“Introducing VocalPy”](https://dael.euracoustics.org/confs/fa2023/data/articles/000403.pdf), and watch the talks [\"VocalPy: a core Python package for acoustic communication research\"](https://www.youtube.com/watch?v=53S5xM6s70g) and [\"VocalPy as a cast study of domain-driven design in scientific Python\"](https://www.youtube.com/watch?v=PtTegIM6m1o). We are actively developing the library to meet your needs and would love to hear your feedback in [our forum](https://forum.vocalpy.org/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
